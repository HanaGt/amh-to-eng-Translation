{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget \"https://raw.githubusercontent.com/MarsPanther/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_E-Bible/amharic.txt\" -O amharic_bible.txt\n",
    "# !wget \"https://raw.githubusercontent.com/MarsPanther/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_E-Bible/english.txt\" -O english_bible.txt\n",
    "\n",
    "# !wget \"https://raw.githubusercontent.com/MarsPanther/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_History/amharic.txt\" -O amharic_history.txt\n",
    "# !wget \"https://raw.githubusercontent.com/MarsPanther/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_History/english.txt\" -O english_history.txt\n",
    "\n",
    "# !wget \"https://raw.githubusercontent.com/MarsPanther/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_Legal/amharic.txt\" -O amharic_legal.txt\n",
    "# !wget \"https://raw.githubusercontent.com/MarsPanther/Amharic-English-Machine-Translation-Corpus/master/Amharic_English_Legal/english.txt\" -O english_legal.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_texts(amharic_files, english_files):\n",
    "#     amharic_data = []\n",
    "#     english_data = []\n",
    "    \n",
    "#     for am_file, en_file in zip(amharic_files, english_files):\n",
    "#         with open(am_file, \"r\", encoding=\"utf-8\") as am_f, open(en_file, \"r\", encoding=\"utf-8\") as en_f:\n",
    "#             amharic_data.extend(am_f.readlines())\n",
    "#             english_data.extend(en_f.readlines())\n",
    "    \n",
    "#     return amharic_data, english_data\n",
    "\n",
    "# # List of all Amharic and English files\n",
    "# amharic_files = [\"amharic_bible.txt\", \"amharic_history.txt\", \"amharic_legal.txt\"]\n",
    "# english_files = [\"english_bible.txt\", \"english_history.txt\", \"english_legal.txt\"]\n",
    "\n",
    "# # Load and combine datasets\n",
    "# amharic_sentences, english_sentences = load_texts(amharic_files, english_files)\n",
    "\n",
    "# # Check if lengths match\n",
    "# assert len(amharic_sentences) == len(english_sentences), \"Mismatch in dataset sizes!\"\n",
    "\n",
    "# # Print sample of the combined data\n",
    "# print(\"Amharic:\", amharic_sentences[0].strip())\n",
    "# print(\"English:\", english_sentences[0].strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from transformers import BertTokenizer\n",
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# # Load tokenizer (can be adjusted if you choose a different model)\n",
    "# # tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "# # Tokenize Amharic and English sentences\n",
    "# def tokenize_data(amharic_sentences, english_sentences):\n",
    "#     amharic_encodings = tokenizer(amharic_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     english_encodings = tokenizer(english_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#     return amharic_encodings, english_encodings\n",
    "\n",
    "# # Tokenize the data\n",
    "# amharic_encodings, english_encodings = tokenize_data(amharic_sentences, english_sentences)\n",
    "\n",
    "# # Check tokenized output\n",
    "# print(amharic_encodings['input_ids'][0])\n",
    "# print(english_encodings['input_ids'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the datasett in the format the model can work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# # from transformers import MarianTokenizer\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# # Load the tokenizer for MarianMT (Amharic-English)\n",
    "# # tokenizer = MarianTokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "\n",
    "# # Define a custom dataset class\n",
    "# class TranslationDataset(Dataset):\n",
    "#     def __init__(self, amharic_file, english_file, tokenizer, max_length=128):\n",
    "#         # Read Amharic and English sentences from files\n",
    "#         self.amharic_lines = open(amharic_file, 'r', encoding='utf-8').readlines()\n",
    "#         self.english_lines = open(english_file, 'r', encoding='utf-8').readlines()\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.amharic_lines)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         amharic_text = self.amharic_lines[idx].strip()\n",
    "#         english_text = self.english_lines[idx].strip()\n",
    "\n",
    "#         # Tokenize Amharic and English sentences\n",
    "#         amharic_tokens = self.tokenizer(amharic_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "#         english_tokens = self.tokenizer(english_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "#         # Extract input IDs and attention masks\n",
    "#         input_ids = amharic_tokens['input_ids'].squeeze()  # Remove the batch dimension\n",
    "#         attention_mask = amharic_tokens['attention_mask'].squeeze()\n",
    "#         labels = english_tokens['input_ids'].squeeze()\n",
    "\n",
    "#         return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = TranslationDataset('amharic_bible.txt', 'english_bible.txt', tokenizer)\n",
    "\n",
    "# # Create a DataLoader\n",
    "# batch_size = 8\n",
    "# train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Example: check the tokenized output of one batch\n",
    "# for batch in train_dataloader:\n",
    "#     print(batch['input_ids'].shape)\n",
    "#     print(batch['attention_mask'].shape)\n",
    "#     print(batch['labels'].shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import MarianMTModel\n",
    "# from torch.optim import AdamW\n",
    "# import torch\n",
    "\n",
    "# # Load the pre-trained MarianMT model\n",
    "# # model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-afa')\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Define optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# # Training function\n",
    "# def train(model, dataloader, optimizer, device, epochs=3):\n",
    "#     model.train()  # Set model to training mode\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = 0\n",
    "#         progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "#         for batch in progress_bar:\n",
    "#             optimizer.zero_grad()  # Zero gradients from the previous step\n",
    "            \n",
    "#             # Move tensors to the appropriate device\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             labels = batch['labels'].to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             # Backward pass\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Update weights\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Update the progress bar\n",
    "#             progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "#         # Print average loss for the epoch\n",
    "#         avg_loss = total_loss / len(dataloader)\n",
    "#         print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # Train the model\n",
    "# train(model, train_dataloader, optimizer, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the trained model\n",
    "# model.save_pretrained('trained_amaric_english_model')\n",
    "# tokenizer.save_pretrained('trained_amaric_english_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluation function\n",
    "# def evaluate(model, dataloader, device):\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     total_loss = 0\n",
    "\n",
    "#     with torch.no_grad():  # Disable gradient calculation\n",
    "#         for batch in dataloader:\n",
    "#             # Move tensors to the appropriate device\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             labels = batch['labels'].to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#             loss = outputs.loss\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     # Print average evaluation loss\n",
    "#     avg_loss = total_loss / len(dataloader)\n",
    "#     print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# # Assuming you have a validation DataLoader for evaluation\n",
    "# # Evaluate the model\n",
    "# evaluate(model, train_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Translation function for inference\n",
    "# def translate(model, tokenizer, text, device):\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "\n",
    "#     # Tokenize the input text\n",
    "#     inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "#     input_ids = inputs['input_ids'].to(device)\n",
    "#     attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "#     # Generate translation\n",
    "#     with torch.no_grad():\n",
    "#         generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, num_beams=5, max_length=128)\n",
    "\n",
    "#     # Decode the generated tokens\n",
    "#     translated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "#     return translated_text\n",
    "\n",
    "# # Example translation\n",
    "# test_sentence = \"እንዴት ነህ?\"\n",
    "# translated = translate(model, tokenizer, test_sentence, device)\n",
    "# print(\"Original:\", test_sentence)\n",
    "# print(\"Translated:\", translated)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
